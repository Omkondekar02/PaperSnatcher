{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fetched successfully!\n",
      "Details fetched for PubMed ID: 40080018\n",
      "Details fetched for PubMed ID: 40079843\n",
      "Details fetched for PubMed ID: 40079842\n",
      "Details fetched for PubMed ID: 40079787\n",
      "Details fetched for PubMed ID: 40079709\n",
      "Details fetched for PubMed ID: 40079707\n",
      "Details fetched for PubMed ID: 40079695\n",
      "Details fetched for PubMed ID: 40079557\n",
      "Details fetched for PubMed ID: 40079429\n",
      "Details fetched for PubMed ID: 40079412\n",
      "Details fetched for PubMed ID: 40079395\n",
      "Details fetched for PubMed ID: 40079266\n",
      "Details fetched for PubMed ID: 40079210\n",
      "Details fetched for PubMed ID: 40079180\n",
      "Details fetched for PubMed ID: 40079164\n",
      "Details fetched for PubMed ID: 40079157\n",
      "Details fetched for PubMed ID: 40079137\n",
      "Details fetched for PubMed ID: 40079074\n",
      "Details fetched for PubMed ID: 40079018\n",
      "Details fetched for PubMed ID: 40078968\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'papers.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 153\u001b[0m\n\u001b[0;32m    150\u001b[0m         save_to_csv(filtered_data)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# Run the main function directly\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[60], line 150\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    147\u001b[0m filtered_data \u001b[38;5;241m=\u001b[39m filter_by_affiliation(papers_data)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# Save filtered data to a CSV file\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m \u001b[43msave_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[60], line 114\u001b[0m, in \u001b[0;36msave_to_csv\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_to_csv\u001b[39m(data):\n\u001b[0;32m    113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save the filtered data to a CSV file.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpapers.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n\u001b[0;32m    115\u001b[0m         fieldnames \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPubMedID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPublication Date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNon-academic Author(s)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompany Affiliation(s)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorresponding Author Email\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    116\u001b[0m         writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictWriter(csvfile, fieldnames\u001b[38;5;241m=\u001b[39mfieldnames)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'papers.csv'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "# Function to fetch papers from PubMed based on the search query\n",
    "def fetch_papers(query: str):\n",
    "    \"\"\"Fetch papers from PubMed based on a search query.\"\"\"\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "    params = {\n",
    "        'db': 'pubmed',\n",
    "        'term': query,  # Search term\n",
    "        'retmax': 20,  # Number of papers to fetch\n",
    "        'retmode': 'xml'  # We want the data in XML format\n",
    "    }\n",
    "    \n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"Data fetched successfully!\")\n",
    "        return response.text  # Return the XML response text\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract PubMed IDs from the fetched data\n",
    "def extract_pubmed_ids(xml_data: str):\n",
    "    \"\"\"Extract PubMed IDs from the XML response.\"\"\"\n",
    "    tree = ElementTree.fromstring(xml_data)\n",
    "    ids = [id_elem.text for id_elem in tree.findall(\".//Id\")]\n",
    "    return ids\n",
    "\n",
    "# Function to fetch detailed information about a specific paper using its PubMed ID\n",
    "def fetch_paper_details(pubmed_id: str):\n",
    "    \"\"\"Fetch detailed information about a paper using its PubMed ID.\"\"\"\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "    params = {\n",
    "        'db': 'pubmed',\n",
    "        'id': pubmed_id,  # PubMed ID of the paper\n",
    "        'retmode': 'xml'  # We want the data in XML format\n",
    "    }\n",
    "    \n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(f\"Details fetched for PubMed ID: {pubmed_id}\")\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Error fetching details for PubMed ID {pubmed_id}: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to parse the detailed paper data and extract necessary information\n",
    "def parse_paper_data(xml_data: str):\n",
    "    \"\"\"Parse the XML data and extract relevant information.\"\"\"\n",
    "    tree = ElementTree.fromstring(xml_data)\n",
    "    \n",
    "    title = tree.find(\".//Item[@Name='Title']\").text if tree.find(\".//Item[@Name='Title']\") is not None else \"N/A\"\n",
    "    pub_date = tree.find(\".//PubDate\").text if tree.find(\".//PubDate\") is not None else \"N/A\"\n",
    "    \n",
    "    authors = []\n",
    "    affiliations = []\n",
    "    corresponding_email = \"N/A\"\n",
    "    \n",
    "    for author in tree.findall(\".//Author\"):\n",
    "        name = author.find(\"Name\").text if author.find(\"Name\") is not None else \"N/A\"\n",
    "        email = author.find(\"Affiliation\").text if author.find(\"Affiliation\") is not None else \"N/A\"\n",
    "        authors.append(name)\n",
    "        affiliations.append(email)\n",
    "        \n",
    "        if author.find(\"Email\") is not None:\n",
    "            corresponding_email = author.find(\"Email\").text\n",
    "    \n",
    "    return {\n",
    "        'Title': title,\n",
    "        'PubmedID': tree.find(\".//PubmedData//ArticleId\").text if tree.find(\".//PubmedData//ArticleId\") is not None else \"N/A\",\n",
    "        'Publication Date': pub_date,\n",
    "        'Authors': authors,\n",
    "        'Affiliations': affiliations,\n",
    "        'Corresponding Author Email': corresponding_email\n",
    "    }\n",
    "\n",
    "# Function to filter authors affiliated with pharmaceutical/biotech companies\n",
    "def filter_by_affiliation(data):\n",
    "    \"\"\"Filter out authors affiliated with pharmaceutical/biotech companies.\"\"\"\n",
    "    company_keywords = ['pharma', 'biotech', 'Pfizer', 'Moderna', 'Johnson & Johnson', 'AstraZeneca', 'Bayer', 'Novartis', 'Sanofi']\n",
    "    \n",
    "    filtered_data = []\n",
    "    for entry in data:\n",
    "        authors = entry['Authors']\n",
    "        affiliations = entry['Affiliations']\n",
    "        \n",
    "        non_academic_authors = []\n",
    "        companies = []\n",
    "        \n",
    "        for i, affiliation in enumerate(affiliations):\n",
    "            if any(keyword.lower() in affiliation.lower() for keyword in company_keywords):\n",
    "                non_academic_authors.append(authors[i])\n",
    "                companies.append(affiliation)\n",
    "        \n",
    "        if non_academic_authors:\n",
    "            filtered_data.append({\n",
    "                'PubMedID': entry['PubmedID'],\n",
    "                'Title': entry['Title'],\n",
    "                'Publication Date': entry['Publication Date'],\n",
    "                'Non-academic Author(s)': \", \".join(non_academic_authors),\n",
    "                'Company Affiliation(s)': \", \".join(companies),\n",
    "                'Corresponding Author Email': entry['Corresponding Author Email']\n",
    "            })\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "# Function to save filtered data to a CSV file\n",
    "def save_to_csv(data):\n",
    "    \"\"\"Save the filtered data to a CSV file.\"\"\"\n",
    "    with open('papers.csv', 'w', newline='') as csvfile:\n",
    "        fieldnames = ['PubMedID', 'Title', 'Publication Date', 'Non-academic Author(s)', 'Company Affiliation(s)', 'Corresponding Author Email']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "        print(\"Filtered data saved to papers.csv.\")\n",
    "\n",
    "# Main function to handle user input and execution\n",
    "def main():\n",
    "    \"\"\"Main function to handle user input and fetch papers.\"\"\"\n",
    "    # Manually set your search query here for testing\n",
    "    query = \"pharmaceutical research\"\n",
    "    \n",
    "    # Fetch papers based on the query\n",
    "    xml_data = fetch_papers(query)\n",
    "    \n",
    "    if xml_data:\n",
    "        # Extract PubMed IDs from the fetched data\n",
    "        pubmed_ids = extract_pubmed_ids(xml_data)\n",
    "        \n",
    "        # Initialize a list to store paper details\n",
    "        papers_data = []\n",
    "        \n",
    "        # Fetch detailed information for each paper using its PubMed ID\n",
    "        for pubmed_id in pubmed_ids:\n",
    "            paper_xml = fetch_paper_details(pubmed_id)\n",
    "            \n",
    "            if paper_xml:\n",
    "                paper_data = parse_paper_data(paper_xml)\n",
    "                papers_data.append(paper_data)\n",
    "        \n",
    "        # Filter papers by author affiliation with pharmaceutical/biotech companies\n",
    "        filtered_data = filter_by_affiliation(papers_data)\n",
    "        \n",
    "        # Save filtered data to a CSV file\n",
    "        save_to_csv(filtered_data)\n",
    "\n",
    "# Run the main function directly\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile README.md\n",
    "# Get Papers List\n",
    "\n",
    "This Python program fetches research papers from PubMed based on a user query. It filters the papers based on whether at least one author is affiliated with a pharmaceutical or biotech company, and returns the results in a CSV file.\n",
    "\n",
    "## Features\n",
    "\n",
    "- Fetch research papers from PubMed API.\n",
    "- Filter results to identify authors affiliated with pharmaceutical or biotech companies.\n",
    "- Output the filtered results to a CSV file with the following columns:\n",
    "  - PubmedID: Unique identifier for the paper.\n",
    "  - Title: Title of the paper.\n",
    "  - Publication Date: Date the paper was published.\n",
    "  - Non-academic Author(s): Names of authors affiliated with non-academic institutions.\n",
    "  - Company Affiliation(s): Names of pharmaceutical/biotech companies.\n",
    "  - Corresponding Author Email: Email address of the corresponding author.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Python 3.13 or higher\n",
    "- Poetry (for dependency management)\n",
    "- Dependencies will be automatically installed via `poetry install`.\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. Clone this repository to your local machine.\n",
    "   \n",
    "   ```bash\n",
    "   git clone https://github.com/your-username/get-papers-list.git\n",
    "   cd get-papers-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "def search_papers(query):\n",
    "    print(f\"Searching for: {query}\")\n",
    "    url = f\"https://api.ncbi.nlm.nih.gov/eutils/esearch.fcgi?db=pubmed&term={query}&retmax=10\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"API call successful.\")\n",
    "        print(\"Response:\", response.text)  # Print the raw response to check XML structure\n",
    "\n",
    "        # Parse the XML response\n",
    "        root = ET.fromstring(response.text)\n",
    "\n",
    "        # Extract the list of IDs\n",
    "        id_list = root.find('IdList').findall('Id')\n",
    "        paper_ids = [id_elem.text for id_elem in id_list]\n",
    "        \n",
    "        # Fetch details for each paper using the IDs (if needed)\n",
    "        return paper_ids\n",
    "    else:\n",
    "        print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def save_papers_to_csv(paper_ids, filename):\n",
    "    if not paper_ids:\n",
    "        print(\"No papers to save.\")\n",
    "        return\n",
    "\n",
    "    # Prepare mock data (or you can fetch detailed info using the paper_ids if necessary)\n",
    "    papers_details = [\n",
    "        {'Title': f'Paper {i+1}', 'Authors': 'Author A, Author B', 'Source': 'Journal XYZ', 'Year': '2021'}\n",
    "        for i in range(len(paper_ids))\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        df = pd.DataFrame(papers_details)\n",
    "        print(f\"Data to be saved:\\n{df}\")  # Debug print to check if data is correct\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Results saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving the CSV: {str(e)}\")\n",
    "\n",
    "def main(query, filename):\n",
    "    print(f\"Fetching papers for query: {query}\")\n",
    "    paper_ids = search_papers(query)\n",
    "    save_papers_to_csv(paper_ids, filename)\n",
    "\n",
    "# Running the function with a test query and output filename\n",
    "query = \"cancer research\"\n",
    "output_filename = \"C:/Users/Admin/Dropbox/My PC (LAPTOP-D6DN0OTQ)/Desktop/papers.csv\"  # Save to Desktop\n",
    "main(query=query, filename=output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_papers(query):\n",
    "    url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "    params = {\n",
    "        'db': 'pubmed',\n",
    "        'term': query,\n",
    "        'retmax': 10,  # Number of results to fetch\n",
    "        'retmode': 'xml'  # Change to 'json' if the response is expected in JSON format\n",
    "    }\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    print(\"API call successful.\")\n",
    "    print(\"Response:\", response.text)  # Print raw response to inspect it\n",
    "\n",
    "    try:\n",
    "        # If the response is in XML format, you might need to parse it as XML instead of JSON\n",
    "        if response.status_code == 200:\n",
    "            return response.text  # Returning raw XML if in XML format\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_papers_from_xml(xml_data):\n",
    "    try:\n",
    "        # Parse the XML data\n",
    "        root = ET.fromstring(xml_data)\n",
    "\n",
    "        # Extract paper IDs from the XML response\n",
    "        paper_ids = root.findall('.//Id')  # Find all the <Id> elements\n",
    "        paper_ids = [id_elem.text for id_elem in paper_ids]\n",
    "\n",
    "        print(f\"Found {len(paper_ids)} papers.\")\n",
    "        return paper_ids\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing XML: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(query, filename):\n",
    "    print(f\"Fetching papers for query: {query}\")\n",
    "    raw_data = search_papers(query)  # Get the raw response\n",
    "    \n",
    "    if raw_data:\n",
    "        # Parse XML if needed\n",
    "        paper_ids = parse_papers_from_xml(raw_data)\n",
    "        \n",
    "        if paper_ids:\n",
    "            save_papers_to_csv(paper_ids, filename)  # Save the data to CSV\n",
    "        else:\n",
    "            print(\"No papers found.\")\n",
    "    else:\n",
    "        print(\"Failed to fetch data.\")\n",
    "\n",
    "# Running the function with the query\n",
    "query = \"cancer research\"\n",
    "output_filename = \"C:/Users/Admin/Dropbox/My PC (LAPTOP-D6DN0OTQ)/Desktop/papers.csv\"  # Adjust path if needed\n",
    "main(query=query, filename=output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(query, filename):\n",
    "    print(f\"Fetching papers for query: {query}\")\n",
    "    raw_data = search_papers(query)  # Get the raw response\n",
    "    \n",
    "    if raw_data:\n",
    "        # Parse XML if needed\n",
    "        paper_ids = parse_papers_from_xml(raw_data)\n",
    "        \n",
    "        if paper_ids:\n",
    "            save_papers_to_csv(paper_ids, filename)  # Save the data to CSV\n",
    "        else:\n",
    "            print(\"No papers found.\")\n",
    "    else:\n",
    "        print(\"Failed to fetch data.\")\n",
    "\n",
    "# Running the function with the query\n",
    "query = \"cancer research\"\n",
    "output_filename = \"C:/Users/Admin/Dropbox/My PC (LAPTOP-D6DN0OTQ)/Desktop/papers.csv\"  # Adjust path if needed\n",
    "main(query=query, filename=output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def fetch_paper_details(paper_id):\n",
    "    url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "    params = {\n",
    "        'db': 'pubmed',\n",
    "        'id': paper_id,\n",
    "        'retmode': 'xml',  # Get the data in XML format\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Parse the XML response\n",
    "        root = ET.fromstring(response.text)\n",
    "        \n",
    "        # Extract paper details\n",
    "        try:\n",
    "            title = root.find('.//ArticleTitle').text if root.find('.//ArticleTitle') else 'N/A'\n",
    "            authors = \", \".join([author.text for author in root.findall('.//Author/LastName')]) if root.findall('.//Author') else 'N/A'\n",
    "            source = root.find('.//Source').text if root.find('.//Source') else 'N/A'\n",
    "            year = root.find('.//PubDate/Year').text if root.find('.//PubDate/Year') else 'N/A'\n",
    "            \n",
    "            return {\n",
    "                \"Title\": title,\n",
    "                \"Authors\": authors,\n",
    "                \"Source\": source,\n",
    "                \"Year\": year\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting paper details: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to fetch details for paper ID: {paper_id}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def save_papers_to_csv(paper_ids, filename):\n",
    "    papers_details = []\n",
    "\n",
    "    # Fetch details for each paper and store them\n",
    "    for paper_id in paper_ids:\n",
    "        print(f\"Fetching details for paper ID: {paper_id}\")\n",
    "        details = fetch_paper_details(paper_id)\n",
    "        \n",
    "        if details:\n",
    "            papers_details.append(details)\n",
    "    \n",
    "    # Convert the list of paper details into a DataFrame and save to CSV\n",
    "    if papers_details:\n",
    "        df = pd.DataFrame(papers_details)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Results saved to {filename}\")\n",
    "    else:\n",
    "        print(\"No paper details found to save.\")\n",
    "\n",
    "# Running the final main function\n",
    "def main(query, filename):\n",
    "    print(f\"Fetching papers for query: {query}\")\n",
    "    raw_data = search_papers(query)  # Get the raw response\n",
    "    \n",
    "    if raw_data:\n",
    "        # Parse XML if needed\n",
    "        paper_ids = parse_papers_from_xml(raw_data)\n",
    "        \n",
    "        if paper_ids:\n",
    "            save_papers_to_csv(paper_ids, filename)  # Save the data to CSV\n",
    "        else:\n",
    "            print(\"No papers found.\")\n",
    "    else:\n",
    "        print(\"Failed to fetch data.\")\n",
    "\n",
    "# Running the function with the query\n",
    "query = \"cancer research\"\n",
    "output_filename = \"C:/Users/Admin/Dropbox/My PC (LAPTOP-D6DN0OTQ)/Desktop/papers.csv\"  # Adjust path if needed\n",
    "main(query=query, filename=output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def save_papers_to_csv(papers, filename):\n",
    "    # If papers contain any data, proceed to save\n",
    "    if papers:\n",
    "        df = pd.DataFrame(papers)\n",
    "        try:\n",
    "            df.to_csv(filename, index=False)  # Save data to CSV\n",
    "            print(f\"Results saved to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving the CSV: {e}\")\n",
    "    else:\n",
    "        print(\"No papers found to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = [\n",
    "    {\"Title\": \"Paper 1\", \"Authors\": \"Author A, Author B\", \"Source\": \"Journal XYZ\", \"Year\": 2021},\n",
    "    {\"Title\": \"Paper 2\", \"Authors\": \"Author A, Author B\", \"Source\": \"Journal XYZ\", \"Year\": 2021}\n",
    "    # Add more papers...\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"cancer research\"\n",
    "output_filename = \"C:/Users/Admin/Dropbox/My PC (LAPTOP-D6DN0OTQ)/Desktop/papers.csv\"\n",
    "main(query=query, filename=output_filename)  # Run your main function to fetch and save the papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Example data (replace this with your actual data)\n",
    "data = {\n",
    "    'Title': ['Paper 1', 'Paper 2', 'Paper 3'],\n",
    "    'Authors': ['Author A, Author B', 'Author A, Author B', 'Author A, Author B'],\n",
    "    'Source': ['Journal XYZ', 'Journal XYZ', 'Journal XYZ'],\n",
    "    'Year': [2021, 2021, 2021]\n",
    "}\n",
    "\n",
    "# Create DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the folder and filename\n",
    "output_directory = \"C:/Users/Admin/Dropbox/My PC (LAPTOP-D6DN0OTQ)\\Desktop\"\n",
    "output_filename = \"papers.csv\"\n",
    "output_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Now save the file\n",
    "try:\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Results saved to {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving the file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
